{
    "contents" : "---\ntitle: \"Homework 4\"\nauthor: \"Hong Xu\"\ndate: \"September 24, 2015\"\noutput: pdf_document\n---\n\n## Problem 1\n```{r}\nload(\"hw1prob1.Rdata\")\nD <- nrow(dtm)\n## a)\n## dtm1\n# first normalize\ndtm1 <- t(apply(dtm, 1, function(x) x/sum(x)))\n# then IDF transformation\ndtm1 <- scale(dtm1, center = FALSE, scale = apply(dtm1, 2, function(x) 1/log(D/sum(x > 0))))\n\n## dtm2\n# first IDF transformation\ndtm2 <- scale(dtm, center = FALSE, scale = apply(dtm, 2, function(x) 1/log(D/sum(x > 0))))\n# then normalize\ndtm2 <- t(apply(dtm2, 1, function(x) x/sum(x)))\n\n\n# compare \nall(dtm1 == dtm2)\n## They are not supposed to be equal. IDF transformation will impose different weights on \n## different terms so the normalized results will not be proportional to the term frequencies.\n## We should always normalize first then perform IDF weighting.\n\n\n## b)\n# just normalize\ndtm3 <- t(apply(dtm, 1, function(x) x/sum(x)))\n# compute the distance\ncomputeDistance <- function(x, q) {\n  # input:\n  # x: a vector of normalized terms for a document\n  # q: a vector of normalized terms for the query\n  sqrt(sum((x - q)^2))\n}\ndist3 <- apply(dtm3, 1, computeDistance, q = dtm3[3,])\n# find the closet\ndist3[order(dist3)]\n## tmnt raph is the closest document\n\n\n## c)\ndist3_dist <- dist(dtm3)\nhc_single <- hclust(dist3_dist, method = \"single\")\nhc_complete <- hclust(dist3_dist, method = \"complete\") \n# plot them together\noldpar <- par(mfrow =c(1,2))\nplot(hc_single , main=\" Single Linkage \", xlab=\"\", sub =\"\", cex =.9)\nplot(hc_complete ,main =\" Complete Linkage \", xlab=\"\", sub =\"\", cex =.9)\npar(oldpar)\n## Obviously the Complete Linkage method gives a more reasonable clustering.\n\n## d)\n# count the total occurance\nword_count <- apply(dtm, 2, sum)\n# print the top 20 and their counts\nword_count[order(word_count, decreasing = TRUE)[1:20]]\n# how many percentage do the top 20 account for?\nsum(word_count[order(word_count, decreasing = TRUE)[1:20]])/sum(word_count)\n# how many top words count for 50% of the occurance?\nL <- length(word_count)\npercentage_i <- rep(0, L)\n# find the count \nfor (i in 1:L) {\n  percentage_i[i] <- sum(word_count[order(word_count, decreasing = TRUE)[1:i]])/sum(word_count)\n\n}\nwhich(signif(percentage_i, 3) == 0.5)\nwhich(signif(percentage_i, 3) == 0.5) / L\n## 253 (or 254) top words (top 3.7% of the total words) account for 50% of the total occurance\n\n## e)\n## Test Zipf's law\n# compute the proportion of occurance to the most common\nordered_count <- word_count[order(word_count, decreasing = TRUE)]\nproportion_i <- ordered_count / ordered_count[1]\n# plot the word count proportion and y = 1/x function\n# use log transformation -log(y) = log(x)\nplot(x = seq(1, L), y = -log(proportion_i), type = \"p\", pch = 16, col = \"blue\", ylim = c(0, 10))\nlines(x = seq(1, L), y = log(seq(1, L)), col = \"red\")\n## It appears that our collection of documents loosely follow the Zipf's law in the first ~200 ranks.\n\n```\n\n## Problem 2\n```{r}\n# Compose A the link matrix\nL <- matrix(rep(0, 100), 10, 10)\nidx <- rbind(c(10, 1), \n             c(3, 2), c(6, 2), \n             c(4, 3), c(10, 3), \n             c(9, 4),\n             c(4, 5), c(10, 5), \n             c(4, 6), \n             c(5, 7), c(8, 7), c(10, 7),\n             c(4, 8),\n             c(8, 9),\n             c(4, 10))\nL[idx] <- 1\n\n## Compute PageRank\nmy_pagerank <- function(L, d=0.85) {\n  ##\n  ## The function that calculate the PageRank vector\n  ## for a given link matrix.\n  ##\n  ## Input:\n  ## L, the link matrix \n  ## d, numerical, the dampening parameter\n  ##\n  ## Output:\n  ## p, the PageRank vector\n  ##\n\n  # compose the matrix to find the PageRank vector\n  M <- diag(colSums(L))\n  MInv <- solve(M)\n  A <- L %*% MInv\n  # build a strong conneted Markov Chain\n  n <- nrow(A)\n  E <- matrix(rep(1, n*n), n, n)\n  A <- ((1 - d) / n) * E + d * A\n  # find the eigenvector\n  A_eig <- eigen(A)\n  PR_vector <- A_eig$vectors[, which(signif(A_eig$values, 6) == 1)]\n  # normalize\n  p <- scale(PR_vector, center = FALSE, \n                     scale = as.numeric(apply(as.matrix(PR_vector), 2, sum)))\n  return(as.numeric(p))\n}\n\nmy_pagerank(L, 0.85)\n## Compute BrokenRank\nmy_pagerank(L, 1)\n## BrokenRank gives us unreasonale results:\n## we are only getting the ranks for page 4, 8, and 9.\n\n```\n\n## Problem 3\n\n## Problem 4\n```{r}\n## a)\nload(\"hw1prob3.Rdata\")\n# reference: ISLR 10.6.1\nCols <- function (vec) {\n  cols=rainbow (length (unique (vec )))\n  return (cols[as.numeric (as.factor (vec))])\n}\n\nhclust_and_plot <- function(x, d, method) {\n  ##\n  ## Input: \n  ## x, the data matrix \n  ## d, the pairwise Euclidean distance \n  ## method, the hierarchical clustering linkage\n  ##\n  ## Returns:\n  ## No returned object\n  ## \n  \n  hc_4 <- hclust(d, method = method)\n  # cut into 4 clusters\n  c_labels <- cutree(hc_4, k = 4)\n  # plot points in x\n  plot(x[,1:2], col =Cols(c_labels), pch =19, xlab =\"X1\",ylab=\"X2\")\n  # plot dendorgram\n  plot(hc_4, col = \"#487AA1\", col.main = \"#45ADA8\", col.lab = \"#7C8071\", \n                      col.axis = \"#F38630\", hang = -1, xlab = \"\")\n\n}\n# run hierarchical agglomerative clustering with single linkage\nhclust_and_plot(x, d, \"single\")\n\n## b)\n# repeat a for complete linkage\nhclust_and_plot(x, d, \"complete\")\n\n## c)\n## Repeat a and b but passing d^2\nhclust_and_plot(x, d^2, \"single\")\nhclust_and_plot(x, d^2, \"complete\")\n\n## A monotone increasing transformation does not change the clustering assignment\n## or the dendrogram for single or complete linkage.\n\n```\n\n\n## d)\nAt step $m \\in {1, \\cdots, N-1}$, $N$ = number of points:\\newline\n\\indent Suppose we have clusters ${1, \\cdots, K_m}$ at step $m$. \\newline\n\\indent First, for $G, H \\in {1, \\cdots, K_m}$, the cluster dissimilarities:\n$$d_{single}(G, H) = \\min\\limits_{g \\in G, h \\in H} h(d_{gh})$$\n\\indent The pair of points $g_m, h_m$ chosen to represent the cluster dissimilarites between $G, H$ would be the same as the one before transformation:\n  $$g_m, h_m = \\underset{g \\in G, h \\in H}{\\arg\\min} h(d_{gh}) = \\underset{g \\in G, h \\in H}{\\arg\\min} d_{gh}$$ \n  \n\\indent Then, the pair of clusters $I_m, J_m$ that has smallest simliarity: \n  $$I, J = \\underset{I, J \\in {1, \\cdots, K_m}}{\\arg\\min} d_{single}(I, J) = \\underset{I, J \\in {1, \\cdots, K_m}}{\\arg\\min} h(d_{i_m, j_m}) = \\underset{I, J \\in {1, \\cdots, K_m}}{\\arg\\min} d_{i_m, j_m}$$\n\\indent So at step $m$, the same pair of clusters $I_m, J_m$ would be chosen to merge as prior to the transformation. \nSince every step we make the same merging choice, the result will be the same. \n\\newline\n\\newline\nFor complete linkage, the idea is the same:\n\\indent At step $m$, the pair of points $g_m, h_m$ chosen to represent the cluster dissimilarites between $G, H$ would be the same as the one before transformation:\n  $$g_m, h_m = \\underset{g \\in G, h \\in H}{\\arg\\max} h(d_{gh}) = \\underset{g \\in G, h \\in H}{\\arg\\max} d_{gh}$$ \nThe rest of the proof is the same as the pair of clusters $I_m, J_m$ is also chosen based on the smallest dissimilairity.\n\n",
    "created" : 1443116829402.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2362051274",
    "id" : "D3389244",
    "lastKnownWriteTime" : 1443130432,
    "path" : "~/Documents/Duke/Term 3/sta 521/hw4/Hong_Xu_Solutions_HW4.Rmd",
    "project_path" : "Hong_Xu_Solutions_HW4.Rmd",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "type" : "r_markdown"
}