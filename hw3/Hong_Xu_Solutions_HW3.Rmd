---
title: "Xu_Hong_solutions_HW3"
author: "Hong Xu"
date: "September 17, 2015"
output: 
  pdf_document:
      includes:
          in_header: multisymbols.sty
---

## 1. 
Prove that the total variance of $\bX$ is the MSE of the mean:
$$E|| \bX - E(\bX) ||^2 = \tr(\var(\bX)).$$ 

Proof:

We can show that the MSE of the mean:  $E|| \bX - E(\bX) ||^2 = E(\sum_i (X_i - E(X_i))^2) = \sum_i E((X_i - E(X_i))^2)$

The covariance of $\bX$: $\var(\bX) = E((\bX - E(\bX))(\bX - E(\bX))^T)$

So the total variance of $\bX$ equals to $\tr(\var(\bX)) = \sum_i E((X_i - E(X_i))(X_i - E(X_i))^T) = \sum_i E((X_i - E(X_i))^2)$, which equals to the MSE of the mean.

## 2.
Suppose $\bY \sim N_n (0 ,I)$ and let $A$ be symmetric. Show that $$\bY^TA\bY \stackrel{dist} {=}\sum_i \lambda_i W_i,$$ where 
$W_i \stackrel{iid}{\sim} \chi_1^2$ and $\lambda_1,\ldots,\lambda_n$ are the eigenvalues of $A.$

Proof:

Since A is symmetric, we have the spectral decomposition of $A = PDP^T$,

where $P$ is orthogonal, $D$ is a diagonal matrix diag($\lambda_1,\ldots,\lambda_n$)

So $\bY^TA\bY = \bY^TPDP^T\bY = (P^T\bY)^TD(P^T\bY)$

and $P^T\bY \sim N_n (P^T0, P^TIP)$, since $P$ is orthogonal, we have $P^T\bY \sim N_n (0, I)$.

With that, we can show $\bY^TA\bY = \bY^TD\bY = \sum_i \sum_j \lambda_{ij} Y_i Y_j = \sum_i \lambda_i Y_i^2$.

And $\bY \sim N_n (0 ,I)$, 

So, $Y_i \sim N_n (0, 1)$, $W_i = Y_i^2 \sim \chi_1^2$.

So that $\bY^TA\bY \stackrel{dist} {=}\sum_i \lambda_i W_i.$


## 3.
### a) 

Show that this model implies that there is a positive covariance for within-state poverty rates over time, $\Cov{Y_{i1}, Y_{i 2}} = \Var{U_i} = \sigma^2_u$.

Proof:

Since $U_i \stackrel{iid}{\sim} \mathcal{N}(0,\sigma^2_u)$ and 
$\epsilon_{it} \stackrel{ind}{\sim}\mathcal{N}(0,\sigma^2_{e_t})$, 

and $Y_{it} = \mu_t + U_{i} + \epsilon_{it}$,

Then, at time t, $E(Y_{it}) = \mu_t + E(U_i) + E(\epsilon_{it}) = \mu_t + 0 + 0 = \mu_t$.

So, $\Cov{Y_{i1}, Y_{i 2}} = E((Y_{i1} - E(Y_{i1}))(Y_{i2} - E(Y_{i2}))) = E((Y_{i1} - \mu_1)(Y_{i2} - \mu_2)) =$
$E((U_i + \epsilon_{i1})(U_i + \epsilon_{i2})) = E(U_i^2 + \epsilon_{i1}U_i + \epsilon_{i2}U_i + \epsilon_{i1}\epsilon_{i2})$
$= E(U_i^2) + E(\epsilon_{i1}U_i) + E(\epsilon_{i2}U_i) + E(\epsilon_{i1}\epsilon_{i2})$.

Since $U_i$ and $\epsilon_{it}$  are independent, it's easy to show the last three expecations are zero. So, $\Cov{Y_{i1}, Y_{i 2}} = E(U_i^2)$

Also, $\var(U_i) = E(U_i^2) - E(U_i)^2 = E(U_i^2)$,

So we have $\Cov{Y_{i1}, Y_{i2}} = E(U_i^2) = \var(U_i) = \sigma^2_\mu$


### b)

Answer:

Since $Y_{it} = \mu_t + U_{i} + \epsilon_{it}$,

$Y_{it} \sim N (\mu_t, \sigma^2_\mu + \sigma^2_{e_t})$.

Obviously $Y_{i1}$ and $Y_{i2}$ follow normaal distribution, so that $\mathbf{Y}_i = (Y_{i1}, Y_{i2})^T$ would be an $\mathcal{MVN}$ distribution.

Its mean vector $\mu$ would be $E(\bY_i) =  (E(Y_{i1}), E(Y_{i2}))^T = (\mu_1, \mu_2)^T$, given the proof we showed in (a).

Its $\Sigma$ would be: 
$$\mathbf{\Sigma} = \left[\begin{array}
{rrr}
\var(Y_{i1}) & \cov(Y_{i1}, Y_{i2}) \\
\cov(Y_{i2}, Y_{i1}) & \var(Y_{i2})
\end{array}\right]
$$
Given the proof in a) $\Cov{Y_{i1}, Y_{i2}} = E(U_i^2) = \var(U_i) = \sigma^2_\mu$, 
$$\mathbf{\Sigma} = \left[\begin{array}
{rrr}
\sigma^2_\mu + \sigma^2_{e_1} & \sigma^2_\mu \\
\sigma^2_\mu & \sigma^2_\mu + \sigma^2_{e_2}
\end{array}\right]
$$

### c)
```{r}
# skip the first row
saipe98 <- read.csv("CPS98T.TXT", skip = 1, sep="")
saipe05 <- read.csv("CPS05T.TXT", skip = 1, sep="")
# for children aged 5-17, CPS poverty is at column cps98.1 or cps05.1
cps98 <- saipe98[, "cps98.1"]
cps05 <- saipe05[, "cps05.1"]
# construct a data frame 
cps <- cbind(cps98, cps05)
```

### d)
```{r}
## find the mean poverty rate 
colMeans(cps)
```

### e)
```{r}
## find the covariance matrix for Y, i.e. cps
cov(cps)
```

### f)
In b) we showed:
$$\mathbf{\Sigma} = \left[\begin{array}
{rrr}
\sigma^2_\mu + \sigma^2_{e_1} & \sigma^2_\mu \\
\sigma^2_\mu & \sigma^2_\mu + \sigma^2_{e_2}
\end{array}\right]
$$
and
$$
\mu = (\mu_1, \mu_2)^T
$$
So,

$\mu_1 = 16.97407$

$\mu_2 = 15.69192$

$\sigma^2_\mu = 27.59969$

$\sigma^2_{e_1} = 39.92843 - \sigma^2_\mu = 39.92843 - 27.59969 = 12.32874$

$\sigma^2_{e_2} = 39.92843 - \sigma^2_\mu = 33.05264 - 27.59969 = 5.45295$



### g)
```{r}
require(mvtnorm)
x.points <- seq(0, 35, length.out = 100)
y.points <- x.points
z <- matrix(0, 100, 100)
mu <- colMeans(cps)
sigma <- cov(cps)

for (i in 1:100) {
  for (j in 1:100) {
    z[i, j] <- dmvnorm(c(x.points[i], y.points[j]), mean = mu, sigma = sigma)
  }
}
contour(x.points, y.points, z)
# add points
points(cps[,1], cps[,2])

## The multivariate normal looks like a good fit to the data
## given the density and the rotation the data points appear.
```


